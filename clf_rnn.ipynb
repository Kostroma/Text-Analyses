{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".widget-label { min-width: 20ex !important; }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, SVG, Image, display_html\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML('''<style>\n",
    ".widget-label { min-width: 20ex !important; }\n",
    "</style>'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1204949, 13), 0.22822210732570425)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid     category                subcategory              title  \\\n",
       "0  10000010    Транспорт      Автомобили с пробегом  Toyota Sera, 1991   \n",
       "1  10000094  Личные вещи  Одежда, обувь, аксессуары   Костюм Steilmann   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "\n",
       "                                               attrs   price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...  150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...    1500        NaN   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape, df.is_blocked.mean())\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Blocked ratio', 0.22822210732570425)\n",
      "('Count:', 1204949)\n"
     ]
    }
   ],
   "source": [
    "print(\"Blocked ratio\",df.is_blocked.mean())\n",
    "print(\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsample\n",
    "#downsample data so that both classes have approximately equal ratios\n",
    "sampled_df = [row for row in df.itertuples() if row.is_blocked == 1]\n",
    "positive_examples = [row for row in df.itertuples() if row.is_blocked == 0]\n",
    "random.shuffle(positive_examples)\n",
    "add_to_samples = len(sampled_df)\n",
    "for i in range(add_to_samples):\n",
    "    sampled_df.append(positive_examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Blocked ratio:', 0.5)\n",
      "('Count:', 549992)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000478</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Рефлекторно-урогинекологический массаж</td>\n",
       "      <td>Комбинированная методика рефлекторных техник м...</td>\n",
       "      <td>{\"Вид услуги\":\"Красота, здоровье\"}</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid category        subcategory  \\\n",
       "0  10000317   Услуги  Предложения услуг   \n",
       "1  10000478   Услуги  Предложения услуг   \n",
       "\n",
       "                                    title  \\\n",
       "0   Поездки на таможню, печать в паспорте   \n",
       "1  Рефлекторно-урогинекологический массаж   \n",
       "\n",
       "                                         description  \\\n",
       "0  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "1  Комбинированная методика рефлекторных техник м...   \n",
       "\n",
       "                                               attrs  price  is_proved  \\\n",
       "0  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...   1500        0.0   \n",
       "1                 {\"Вид услуги\":\"Красота, здоровье\"}   1000        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           1           0           0         0         1.43  \n",
       "1           1           0           0         0         1.19  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(df.columns.values)\n",
    "new_cols = ['id']\n",
    "for item in cols:\n",
    "    new_cols.append(item)\n",
    "df = pd.DataFrame(sampled_df, columns = new_cols)\n",
    "del df['id']\n",
    "print(\"Blocked ratio:\",df.is_blocked.mean())\n",
    "print(\"Count:\",len(df))\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA2WytG5pajZ3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d7cdb3990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "min_count = 10\n",
    "tokens = []\n",
    "for token in token_counts:\n",
    "    if token_counts[token] >= 10:\n",
    "        tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 87940\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) > 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 15)\n",
      "Поездки на таможню, печать в паспорте -> [43320 14749 55452 82157 80237 17397     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [ 8412     0 30493     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [28859 23476     0  3674 33977     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "categories = [{\"category\": row.category, \"subcategory\": row.subcategory} for row in df.itertuples()]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>price</th>\n",
       "      <th>category=Бытовая электроника</th>\n",
       "      <th>category=Для бизнеса</th>\n",
       "      <th>category=Для дома и дачи</th>\n",
       "      <th>category=Животные</th>\n",
       "      <th>category=Личные вещи</th>\n",
       "      <th>category=Недвижимость</th>\n",
       "      <th>...</th>\n",
       "      <th>subcategory=Резюме</th>\n",
       "      <th>subcategory=Ремонт и строительство</th>\n",
       "      <th>subcategory=Собаки</th>\n",
       "      <th>subcategory=Спорт и отдых</th>\n",
       "      <th>subcategory=Телефоны</th>\n",
       "      <th>subcategory=Товары для детей и игрушки</th>\n",
       "      <th>subcategory=Товары для животных</th>\n",
       "      <th>subcategory=Товары для компьютера</th>\n",
       "      <th>subcategory=Фототехника</th>\n",
       "      <th>subcategory=Часы и украшения</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   phones_cnt  emails_cnt  urls_cnt  price  category=Бытовая электроника  \\\n",
       "0           0           0         0   1500                           0.0   \n",
       "1           0           0         0   1000                           0.0   \n",
       "2           0           0         0      0                           0.0   \n",
       "3           1           1         0      0                           0.0   \n",
       "4           0           0         0     18                           0.0   \n",
       "\n",
       "   category=Для бизнеса  category=Для дома и дачи  category=Животные  \\\n",
       "0                   0.0                       0.0                0.0   \n",
       "1                   0.0                       0.0                0.0   \n",
       "2                   0.0                       0.0                0.0   \n",
       "3                   0.0                       0.0                0.0   \n",
       "4                   0.0                       0.0                0.0   \n",
       "\n",
       "   category=Личные вещи  category=Недвижимость              ...               \\\n",
       "0                   0.0                    0.0              ...                \n",
       "1                   0.0                    0.0              ...                \n",
       "2                   0.0                    0.0              ...                \n",
       "3                   0.0                    0.0              ...                \n",
       "4                   1.0                    0.0              ...                \n",
       "\n",
       "   subcategory=Резюме  subcategory=Ремонт и строительство  subcategory=Собаки  \\\n",
       "0                 0.0                                 0.0                 0.0   \n",
       "1                 0.0                                 0.0                 0.0   \n",
       "2                 0.0                                 0.0                 0.0   \n",
       "3                 0.0                                 0.0                 0.0   \n",
       "4                 0.0                                 0.0                 0.0   \n",
       "\n",
       "   subcategory=Спорт и отдых  subcategory=Телефоны  \\\n",
       "0                        0.0                   0.0   \n",
       "1                        0.0                   0.0   \n",
       "2                        0.0                   0.0   \n",
       "3                        0.0                   0.0   \n",
       "4                        0.0                   0.0   \n",
       "\n",
       "   subcategory=Товары для детей и игрушки  subcategory=Товары для животных  \\\n",
       "0                                     0.0                              0.0   \n",
       "1                                     0.0                              0.0   \n",
       "2                                     0.0                              0.0   \n",
       "3                                     0.0                              0.0   \n",
       "4                                     0.0                              0.0   \n",
       "\n",
       "   subcategory=Товары для компьютера  subcategory=Фототехника  \\\n",
       "0                                0.0                      0.0   \n",
       "1                                0.0                      0.0   \n",
       "2                                0.0                      0.0   \n",
       "3                                0.0                      0.0   \n",
       "4                                0.0                      0.0   \n",
       "\n",
       "   subcategory=Часы и украшения  \n",
       "0                           0.0  \n",
       "1                           0.0  \n",
       "2                           0.0  \n",
       "3                           0.0  \n",
       "4                           0.0  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]\n",
    "df_non_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_non_text_features(df):\n",
    "    categories = []\n",
    "    data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "    categories = [{\"category\": row.category, \"subcategory\": row.subcategory} for row in df.itertuples()]\n",
    "    df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n",
    "    vectorizer = DictVectorizer(sparse=False)\n",
    "    cat_one_hot = vectorizer.fit_transform(categories)\n",
    "    cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)\n",
    "    df_non_text = pd.merge(\n",
    "        df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    "    )\n",
    "    del df_non_text[\"key_0\"]\n",
    "    return df_non_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(\n",
    "title_tokens, desc_tokens, df_non_text, target)\n",
    "data_tuple = (title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = False #save\n",
    "read_prepared_data = True #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"готово\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print \"done\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412494 412494 412494 412494\n",
      "137498 137498 137498 137498\n"
     ]
    }
   ],
   "source": [
    "print len(desc_tr), len(title_tr), len(nontext_tr), len(target_tr)\n",
    "print len(desc_ts), len(title_ts), len(nontext_ts), len(target_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandero/anaconda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412494, 15)\n"
     ]
    }
   ],
   "source": [
    "print(title_tr.shape)\n",
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "hidden_layers = 60\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn_pre = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=64)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn_pre, hidden_layers, grad_clipping=100, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "desc_a, desc_b = lasagne.layers.get_output_shape(descr_nn)[1:]\n",
    "descr_nn = lasagne.layers.ReshapeLayer(descr_nn, (-1, desc_a * desc_b))\n",
    "\n",
    "# Titles\n",
    "title_nn_pre = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=64)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn_pre, hidden_layers, grad_clipping=100, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "title_a, title_b = lasagne.layers.get_output_shape(title_nn)[1:]\n",
    "title_nn = lasagne.layers.ReshapeLayer(title_nn, (-1, title_a * title_b))\n",
    "\n",
    "# Non-sequence\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, hidden_layers, W = lasagne.init.Normal(), nonlinearity=lasagne.nonlinearities.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 900)\n",
      "(None, 9000)\n",
      "(None, 60)\n"
     ]
    }
   ],
   "source": [
    "print(lasagne.layers.get_output_shape(title_nn))\n",
    "print(lasagne.layers.get_output_shape(descr_nn))\n",
    "print(lasagne.layers.get_output_shape(cat_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.concat([descr_nn, title_nn, cat_nn]) #<merge three layers into one (e.g. lasagne.layers.concat) >                                  \n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn,20,nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "nn = lasagne.layers.DropoutLayer(nn,p=0.1)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.nesterov_momentum(loss, weights, learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction,target_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates,\n",
    "                           on_unused_input='warn')\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction],\n",
    "                           on_unused_input='warn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",10)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.23318696699\n",
      "\tacc: 0.894901960784\n",
      "\tauc: 0.942508305648\n",
      "\tap@k: 0.997930913298\n",
      "Val:\n",
      "\tloss: 0.22927643824\n",
      "\tacc: 0.896078431373\n",
      "\tauc: 0.955732200834\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.210971251452\n",
      "\tacc: 0.907058823529\n",
      "\tauc: 0.946495007456\n",
      "\tap@k: 0.988802410808\n",
      "Val:\n",
      "\tloss: 0.216659721091\n",
      "\tacc: 0.901176470588\n",
      "\tauc: 0.95525894412\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.220917736277\n",
      "\tacc: 0.903137254902\n",
      "\tauc: 0.941876366016\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.221239285026\n",
      "\tacc: 0.895294117647\n",
      "\tauc: 0.955742756156\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.209427582025\n",
      "\tacc: 0.904705882353\n",
      "\tauc: 0.947202836782\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.227909049297\n",
      "\tacc: 0.896862745098\n",
      "\tauc: 0.951967914596\n",
      "\tap@k: 0.993482060983\n",
      "Train:\n",
      "\tloss: 0.217950074523\n",
      "\tacc: 0.902352941176\n",
      "\tauc: 0.938688684532\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.21919516485\n",
      "\tacc: 0.896078431373\n",
      "\tauc: 0.951777618061\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.213164054301\n",
      "\tacc: 0.90431372549\n",
      "\tauc: 0.946068528543\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.217716901464\n",
      "\tacc: 0.901568627451\n",
      "\tauc: 0.956353073187\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.198903267959\n",
      "\tacc: 0.910980392157\n",
      "\tauc: 0.947482352941\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.225987537544\n",
      "\tacc: 0.89568627451\n",
      "\tauc: 0.953507270198\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.212857794064\n",
      "\tacc: 0.904705882353\n",
      "\tauc: 0.945612668744\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.210061353091\n",
      "\tacc: 0.905490196078\n",
      "\tauc: 0.955205738117\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.188785858313\n",
      "\tacc: 0.918431372549\n",
      "\tauc: 0.954619397129\n",
      "\tap@k: 0.997367804532\n",
      "Val:\n",
      "\tloss: 0.222117852435\n",
      "\tacc: 0.897254901961\n",
      "\tauc: 0.957369572385\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.218467437099\n",
      "\tacc: 0.903137254902\n",
      "\tauc: 0.944563558298\n",
      "\tap@k: 0.981310136772\n",
      "Val:\n",
      "\tloss: 0.213917858697\n",
      "\tacc: 0.903137254902\n",
      "\tauc: 0.954835899729\n",
      "\tap@k: 0.993110037174\n",
      "Train:\n",
      "\tloss: 0.202347218195\n",
      "\tacc: 0.909019607843\n",
      "\tauc: 0.951582816143\n",
      "\tap@k: 0.98829837855\n",
      "Val:\n",
      "\tloss: 0.210764702289\n",
      "\tacc: 0.904705882353\n",
      "\tauc: 0.957593633103\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.210552050647\n",
      "\tacc: 0.904705882353\n",
      "\tauc: 0.947559315424\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.20895637136\n",
      "\tacc: 0.905490196078\n",
      "\tauc: 0.954835427402\n",
      "\tap@k: 0.984876051157\n",
      "Train:\n",
      "\tloss: 0.21263553002\n",
      "\tacc: 0.903137254902\n",
      "\tauc: 0.948626528258\n",
      "\tap@k: 0.967004576734\n",
      "Val:\n",
      "\tloss: 0.196285889155\n",
      "\tacc: 0.914117647059\n",
      "\tauc: 0.964464240017\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.184529438661\n",
      "\tacc: 0.917647058824\n",
      "\tauc: 0.95560022162\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.21251469911\n",
      "\tacc: 0.902745098039\n",
      "\tauc: 0.958644599955\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.207156661566\n",
      "\tacc: 0.905490196078\n",
      "\tauc: 0.940664825881\n",
      "\tap@k: 0.994730348121\n",
      "Val:\n",
      "\tloss: 0.212823388862\n",
      "\tacc: 0.902745098039\n",
      "\tauc: 0.955785319303\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.211052845334\n",
      "\tacc: 0.90431372549\n",
      "\tauc: 0.948066308879\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.205823183384\n",
      "\tacc: 0.903137254902\n",
      "\tauc: 0.957731320771\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.199829409919\n",
      "\tacc: 0.910588235294\n",
      "\tauc: 0.95140420264\n",
      "\tap@k: 0.96744713813\n",
      "Val:\n",
      "\tloss: 0.200456882624\n",
      "\tacc: 0.906274509804\n",
      "\tauc: 0.958088107175\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.203800015096\n",
      "\tacc: 0.911764705882\n",
      "\tauc: 0.943709334488\n",
      "\tap@k: 0.984876051157\n",
      "Val:\n",
      "\tloss: 0.221890565638\n",
      "\tacc: 0.897254901961\n",
      "\tauc: 0.955989603359\n",
      "\tap@k: 0.954522537963\n",
      "Train:\n",
      "\tloss: 0.174162263315\n",
      "\tacc: 0.922745098039\n",
      "\tauc: 0.956366236162\n",
      "\tap@k: 0.995864288041\n",
      "Val:\n",
      "\tloss: 0.229468391087\n",
      "\tacc: 0.894901960784\n",
      "\tauc: 0.945619764654\n",
      "\tap@k: 0.989764176907\n",
      "Train:\n",
      "\tloss: 0.201629352957\n",
      "\tacc: 0.910196078431\n",
      "\tauc: 0.944554422555\n",
      "\tap@k: 0.978700595149\n",
      "Val:\n",
      "\tloss: 0.17433400629\n",
      "\tacc: 0.917254901961\n",
      "\tauc: 0.967533443004\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.199897929692\n",
      "\tacc: 0.913725490196\n",
      "\tauc: 0.942804273685\n",
      "\tap@k: 0.98829837855\n",
      "Val:\n",
      "\tloss: 0.204572459186\n",
      "\tacc: 0.906274509804\n",
      "\tauc: 0.958130503477\n",
      "\tap@k: 0.989764176907\n",
      "Train:\n",
      "\tloss: 0.21790922852\n",
      "\tacc: 0.900392156863\n",
      "\tauc: 0.941076497362\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.203005361174\n",
      "\tacc: 0.904705882353\n",
      "\tauc: 0.957551585653\n",
      "\tap@k: 0.973101773779\n",
      "Train:\n",
      "\tloss: 0.205626402855\n",
      "\tacc: 0.907058823529\n",
      "\tauc: 0.946337243891\n",
      "\tap@k: 0.987238752113\n",
      "Val:\n",
      "\tloss: 0.212616434079\n",
      "\tacc: 0.901568627451\n",
      "\tauc: 0.959472430325\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.200009319891\n",
      "\tacc: 0.91137254902\n",
      "\tauc: 0.944778461538\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.207055846327\n",
      "\tacc: 0.903529411765\n",
      "\tauc: 0.957002329174\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.18836016351\n",
      "\tacc: 0.920392156863\n",
      "\tauc: 0.952527360852\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.201276766817\n",
      "\tacc: 0.907450980392\n",
      "\tauc: 0.956677101866\n",
      "\tap@k: 0.973597805525\n",
      "Train:\n",
      "\tloss: 0.189895122664\n",
      "\tacc: 0.911764705882\n",
      "\tauc: 0.953502742518\n",
      "\tap@k: 0.995864288041\n",
      "Val:\n",
      "\tloss: 0.189234711747\n",
      "\tacc: 0.910588235294\n",
      "\tauc: 0.962347074108\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.217309628213\n",
      "\tacc: 0.899215686275\n",
      "\tauc: 0.945788310224\n",
      "\tap@k: 0.991526489377\n",
      "Val:\n",
      "\tloss: 0.204972740429\n",
      "\tacc: 0.90431372549\n",
      "\tauc: 0.957033419887\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.195820970542\n",
      "\tacc: 0.913333333333\n",
      "\tauc: 0.953755941485\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.203671124897\n",
      "\tacc: 0.907843137255\n",
      "\tauc: 0.963214178495\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.190913770221\n",
      "\tacc: 0.914117647059\n",
      "\tauc: 0.950696245515\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.212139655849\n",
      "\tacc: 0.904705882353\n",
      "\tauc: 0.953810740052\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.181385539079\n",
      "\tacc: 0.917647058824\n",
      "\tauc: 0.951718232506\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.192271830167\n",
      "\tacc: 0.910980392157\n",
      "\tauc: 0.958254176059\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.196449694914\n",
      "\tacc: 0.91137254902\n",
      "\tauc: 0.945527198239\n",
      "\tap@k: 0.960453550479\n",
      "Val:\n",
      "\tloss: 0.20455263299\n",
      "\tacc: 0.904705882353\n",
      "\tauc: 0.957544254108\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.196597449798\n",
      "\tacc: 0.913333333333\n",
      "\tauc: 0.946119269763\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.207474916045\n",
      "\tacc: 0.905882352941\n",
      "\tauc: 0.955964073146\n",
      "\tap@k: 0.970079333598\n",
      "Train:\n",
      "\tloss: 0.183651123786\n",
      "\tacc: 0.917647058824\n",
      "\tauc: 0.949968742001\n",
      "\tap@k: 0.98829837855\n",
      "Val:\n",
      "\tloss: 0.221417890444\n",
      "\tacc: 0.898431372549\n",
      "\tauc: 0.956457911488\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.188585844253\n",
      "\tacc: 0.914509803922\n",
      "\tauc: 0.948326329838\n",
      "\tap@k: 0.987777545217\n",
      "Val:\n",
      "\tloss: 0.189104920534\n",
      "\tacc: 0.912549019608\n",
      "\tauc: 0.962401281517\n",
      "\tap@k: 0.981408415724\n",
      "Train:\n",
      "\tloss: 0.201437233423\n",
      "\tacc: 0.906666666667\n",
      "\tauc: 0.947547833534\n",
      "\tap@k: 0.998474432657\n",
      "Val:\n",
      "\tloss: 0.224663440905\n",
      "\tacc: 0.894509803922\n",
      "\tauc: 0.95340776849\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.19166477091\n",
      "\tacc: 0.914117647059\n",
      "\tauc: 0.951725113818\n",
      "\tap@k: 0.994200546713\n",
      "Val:\n",
      "\tloss: 0.209294460888\n",
      "\tacc: 0.901568627451\n",
      "\tauc: 0.955084220164\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.185143800616\n",
      "\tacc: 0.914901960784\n",
      "\tauc: 0.9551734901\n",
      "\tap@k: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val:\n",
      "\tloss: 0.211046944459\n",
      "\tacc: 0.903529411765\n",
      "\tauc: 0.955027032746\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.168401252657\n",
      "\tacc: 0.924705882353\n",
      "\tauc: 0.963325078066\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.214543252383\n",
      "\tacc: 0.897647058824\n",
      "\tauc: 0.956058558212\n",
      "\tap@k: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 50\n",
    "minibatches_per_epoch = 50\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr.values,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:\n",
    "            break\n",
    "        #print b_desc.shape, b_title.shape, b_cat.shape, b_y.shape\n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c += 1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "#if i % 20 == 0:\n",
    "    print \"Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_ts.values,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: \n",
    "            break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "        \n",
    "#if i % 20 == 0:\n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.213776332477\n",
      "\tacc: 0.900225536559\n",
      "\tauc: 0.951652897929\n",
      "\tap@k: 0.9962958859\n",
      "\n",
      "AUC:\n",
      "\tСойдёт, хотя можно ещё поднажать (ok)\n",
      "\n",
      "Accuracy:\n",
      "\tВсё ок (ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tЗасабмить на kaggle! (great) \n",
      "\t Нет, ну честно - выкачай avito_test.tsv, засабмить и скажи, что вышло.\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr.values,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
